---
title: "Penalized Regression: Examples"
author: "Peter Freeman"
date: "Summer 2019"
output: 
  slidy_presentation:
    font_adjustment: -1
---

Lasso: Regression Example
===

```{r}
load(url("https://raw.githubusercontent.com/pefreeman/36-290/master/EXAMPLE_DATASETS/PHOTO_MORPH/photo_morph.Rdata"))
```
The `load()` function places two stored variables into the global environment: <tt>predictors</tt>, a data frame with 16 measurements for each of 3,419 galaxies, and <tt>response</tt>, a vector with 3,419 redshifts (i.e., metrics of physical distance). Because lasso (and ridge regression) is more useful/effective when $n \sim p$, we will analyze only the first 70 objects.
```{r}
pred.train = predictors[1:50,]
pred.test  = predictors[51:70,]
resp.train = response[1:50]
resp.test  = response[51:70]
```

Lasso: Regression Example
===

Here we utilize the `glmnet` package. The functions of `glmnet` are a tad "prickly" in that they don't abide by the usual rules that apply to model functions in `R`, as you will see. Note that to perform ridge regression, one can perform all the same steps below, except changing the argument `alpha=1` to `alpha=0`. Also note that one can apply lasso and ridge regression in the context of, e.g., binary classification by adding the argument `family="binomial"` to `glmnet()` function calls.
```{r fig.height=6,fig.width=6,fig.align="center"}
if ( require(glmnet) == FALSE ) {
  install.packages("glmnet",repos="https://cloud.r-project.org")
  library(glmnet)
}
x = model.matrix(resp.train~.,pred.train)[,-1]
y = resp.train
out.lasso = glmnet(x,y,alpha=1)  # alpha = 0: ridge regression; alpha = 1: lasso
plot(out.lasso,xvar="lambda")
```

Lasso: Regression Example
===

What is the optimal value of $\lambda$?
```{r fig.height=6,fig.width=6,fig.align="center"}
set.seed(301)  # cv.glmnet() performs random sampling...so set the seed!
cv = cv.glmnet(x,y,alpha=1)
plot(cv)
cv$lambda.min
coef(out.lasso,cv$lambda.min)
```

Lasso: Regression Example
===

```{r}
x.test    = model.matrix(resp.test~.,pred.test)[,-1]
resp.pred = predict(out.lasso,s=cv$lambda.min,newx=x.test)
mean((resp.pred-resp.test)^2)
```

Lasso: Classification Example
===

The dataset that we read in below contains magnitude and redshift data for 500 quasars and 500 stars. The idea is to learn a classifier that can discriminate between quasars and stars with a low misclassification rate.
```{r}
df = read.csv("https://raw.githubusercontent.com/pefreeman/PSU_2019/master/StarQuasar.csv")
dim(df)
names(df)
set.seed(202)
s = sample(nrow(df),0.7*nrow(df))
pred.train = df[s,1:5]                                  # don't include redshift or redshift error!
pred.test  = df[-s,1:5]
resp.train = df[s,8]
resp.test  = df[-s,8]
```

Lasso: Classification Example
===

```{r fig.height=4.5,fig.width=4.5,fig.align="center"}
x = model.matrix(resp.train~.,pred.train)[,-1]
y = resp.train
out.lasso = glmnet(x,y,alpha=1,family="binomial")  # note the "family" argument
plot(out.lasso,xvar="lambda")
```

Lasso: Classification Example
===

What is the optimal value of $\lambda$? (Here, it is very small...meaning there is no reason to favor lasso over logistic regression. This is no surprising given the relative size of the training set [700] to the number of predictors [5].)
```{r fig.height=4.5,fig.width=4.5,fig.align="center"}
set.seed(302)
cv = cv.glmnet(x,y,alpha=1,family="binomial")
plot(cv)
cv$lambda.min
coef(out.lasso,cv$lambda.min)
```

Lasso: Classification Example
===

```{r}
x.test    = model.matrix(resp.test~.,pred.test)[,-1]
resp.prob = predict(out.lasso,s=cv$lambda.min,newx=x.test,type="response")
resp.pred = ifelse(resp.prob>0.5,"STAR","QSO")
mean(resp.pred!=resp.test) # basically the same as for logistic regression
table(resp.pred,resp.test)
```

